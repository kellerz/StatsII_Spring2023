\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 12, 2023}
\author{Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Sunday February 12, 2023. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq x) \frac{\sqrt {2\pi}}{x} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8x^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:

\begin{lstlisting}[language=R]
	# create empirical distribution of observed data
	ECDF <- ecdf(data)
	empiricalCDF <- ECDF(data)
	# generate test statistic
	D <- max(abs(empiricalCDF - pnorm(data))) \end{lstlisting}

\vspace{1in}
\section*{Question 1 Answer}
\noindent Using R, the following function was created:
\lstinputlisting[language=R, firstline=1, lastline=35]{code/part1.R}
\noindent Using the function mentioned above to generate 1,000  Cauchy random variables, this data was then used to test for normality using the "ksfunc()" function and produced the following output:
 
\begin{verbatim}
	stat      p.value         func
	1 0.1347281 2.432277e-08 1.051657e-27
\end{verbatim}

\noindent The "stat" value indicates the D statistic obtained from the first formula, while the "p.value" number indicates the P-value from the test obtained from R's built in ks.test() function. With the p-value being less than .05, we can reject the null hypothesis that the two CDF's are the same, meaning the data is not normal. \\ 
\noindent The "func" stat is meant to be the same as the "p.value" stat. Unfortunately, I could not implement the KS PDF function provided above into R to calculate the p-value. I tried to create the infinite sequence with a tolerance of (\texttt{1e-06}), but the first term seemed to be lower than the tolerance, so the function would stop running after the first term of the sequence.
	 
\vspace{1in}
\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}
\lstinputlisting[language=R, firstline=51,lastline=53]{code/PS1.R}

\vspace{1in}
\section*{Question 2 Answer:} 
\noindent First, the linear likelihood function was created, which finds the log of the likelihood of the PDF of the linear function, being
$$\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{y_{i}-x_{i}\beta}{2\sigma^{2}}}$$
\noindent This was done using the \texttt{linearl} function as shown below:
\lstinputlisting[language=R, firstline=3,lastline=11]{code/part2.R}
\noindent The optimisation function in R was then used to find the MLE of the parameters, by utilising a Hessian matrix and the Newton-Raphson algorithm. The data was created using the code in the above question and passed through the \texttt{optim} function.
\lstinputlisting[language=R, firstline=21,lastline=27]{code/part2.R}
\noindent This gave the following output:
\begin{Verbatim}
	[1] 0.1391893 0.2516985 1.4395471
\end{Verbatim}
\noindent An OLS linear regression was then run on the same data using \texttt{lm(data\_ex\$y $\sim$ data\_ex\$x)}, which generated the following table:
\begin{table}[!htbp] \centering 
	\caption{Regression} 
	\label{} 
	\begin{tabular}{@{\extracolsep{5pt}}lc} 
		\\[-1.8ex]\hline 
		\hline \\[-1.8ex] 
		& \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
		\cline{2-2} 
		\\[-1.8ex] & y \\ 
		\hline \\[-1.8ex] 
		x & 0.252$^{***}$ \\ 
		& (0.042) \\ 
		& \\ 
		Constant & 0.139 \\ 
		& (0.253) \\ 
		& \\ 
		\hline \\[-1.8ex] 
		Observations & 200 \\ 
		R$^{2}$ & 0.156 \\ 
		Adjusted R$^{2}$ & 0.152 \\ 
		Residual Std. Error & 1.447 (df = 198) \\ 
		F Statistic & 36.629$^{***}$ (df = 1; 198) \\ 
		\hline 
		\hline \\[-1.8ex] 
		\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
	\end{tabular} 
\end{table} 
\noindent As shown, the X value, or $\beta$0 value, in the table corresponds to the second value in the output from the \texttt{linear.MLE} output, while the Constant, or $\beta$1 value corresponds to the first value of the output. This displays how OLS and MLS give the same output for these types of linear equations.
\end{document}
